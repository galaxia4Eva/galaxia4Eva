# Lecture notes

---

#link: https://youtu.be/rmVRLeJRkl4

---

## Questions
1. **word2Vec**: 
    1. is there _cyllable2Vec_? what if vector components are not just real numbers, but posterior distributions? Bayesian embeddings? Softmax!
    2. Gensim embeddings, vector arithmetic — language B?
    3. same word as center and context?
    4. Is it like norming a vector space in some sence? X^-1^T * X^-1 ?
        - it does not capture valences, antonyms occur in the same context. it just predicts words in context
    5. There are more efficient ways to optimise objective functions. See next lecture?
    6. Genetic algorithms for optimisation of objective function?
1. CS224S — [speech recognition course](https://web.stanford.edu/class/cs224s/)